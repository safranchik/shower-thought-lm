{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_PATH = os.getcwd()\n",
    "DATA_PATH = BASE_PATH + '/data/'\n",
    "MODEL_PATH = BASE_PATH + '/models/'\n",
    "\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "cll2ER9bJOfI",
    "outputId": "55ab3ad1-cad8-409c-fe5a-f5d9875c751d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pdb\n",
    "seed_model(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('using device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "qABsHLZ_IAvc"
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "XqDIlkOEtfX2",
    "outputId": "80165d93-20c6-43e1-b793-513517a5db98"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class GPT2Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset containing the GPT2 tokens from the subreddit\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_file, tokenizer, gpt2_type=\"gpt2\", max_length=768):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "\n",
    "        with open(DATA_PATH+input_file) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        for post in data:\n",
    "            encodings_dict = tokenizer('<|startoftext|> '+ post + ' <|endoftext|>', \n",
    "                                       truncation=True, \n",
    "                                       max_length=max_length, \n",
    "                                       padding=\"max_length\")\n",
    "\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx] \n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', \n",
    "                                          bos_token='<|startoftext|>', \n",
    "                                          eos_token='<|endoftext|>', \n",
    "                                          pad_token='<|pad|>')\n",
    "\n",
    "shower_thoughts_dataset = GPT2Dataset('posts.json', tokenizer, max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "35iHMp7Ub7Pm"
   },
   "source": [
    "# Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "kJwy3E4hb6GG"
   },
   "outputs": [],
   "source": [
    "def seed_model(seed=7):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "seed_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "-5CW80AAbeD9"
   },
   "source": [
    "# Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "lM-djyU2bbfk"
   },
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data =  torch.utils.data.random_split(shower_thoughts_dataset, [12846, 1000, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Training Samples: {} - Validation Samples: {} - Test Samples: {}'\n",
    "      .format(len(train_data), len(validation_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yag4NQqnSuzK"
   },
   "source": [
    "# GPT-2 Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2 # do not modify - can run into out-of-memory issues\n",
    "EPOCHS = 5\n",
    "LR = 5e-5\n",
    "WARMUP_STEPS = 1e2\n",
    "EPS = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=BATCH_SIZE)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=RandomSampler(validation_data), batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_data, sampler=RandomSampler(test_data), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dbgjEkMIl6m5",
    "outputId": "5c080d9e-bb5c-4966-bf30-89e62d72827c"
   },
   "outputs": [],
   "source": [
    "LOAD_FROM_DISK = True\n",
    "\n",
    "MODEL_NAME = \"final_model_epoch_3\"\n",
    "\n",
    "# Loads GPT2 configuration. We also don't want to output hidden states (not seq2seq).\n",
    "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "\n",
    "\n",
    "if not LOAD_FROM_DISK:\n",
    "    # Instantiates a new pretrained GPT2 LM\n",
    "    print('Loading base gpt model.')\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
    "else:\n",
    "    # TODO: load GPT-2 LM model from existing pre-trained model\n",
    "    print('Loading model from disk.')\n",
    "    model = GPT2LMHeadModel.from_pretrained(MODEL_PATH + MODEL_NAME + '.pt')\n",
    "\n",
    "\n",
    "# need to reisize token embeddings sinze we added new tokens to the embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n",
    "\n",
    "# We use huuggingface's AdamW optimizer, as opposed to pytorch's\n",
    "optimizer = AdamW(model.parameters(), lr=LR, eps=EPS)\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "\n",
    "# Create the learning rate scheduler which changes the learning rate \n",
    "# as the training loop progresses\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = WARMUP_STEPS, \n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sample(model, n=10):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    prompt = \"<|startoftext|>\" # initializes sampling with beginning-of-sentence token\n",
    "    generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)\n",
    "\n",
    "    sample_outputs = model.generate(bos_token_id=generated,\n",
    "                                    do_sample=True,   \n",
    "                                    top_k=50, \n",
    "                                    max_length = 100,\n",
    "                                    top_p=0.95,\n",
    "                                    num_return_sequences=n)\n",
    "\n",
    "    for i, sample_output in enumerate(sample_outputs):\n",
    "        print(\"{}: {}\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
    "        \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "IsoERLCYcTTR"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "WSvErUmKs3TT",
    "outputId": "0ed021c5-3b9c-4c85-ff89-500370d834c3"
   },
   "outputs": [],
   "source": [
    "\n",
    "training_stats = []\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \"\"\" Training \"\"\"\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch + 1, EPOCHS))\n",
    "    print('Training...')\n",
    "\n",
    "    train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        in_features = batch[0].to(device)\n",
    "        labels = batch[0].to(device)\n",
    "        mask = batch[1].to(device)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(in_features, labels=labels, attention_mask=mask, token_type_ids=None)\n",
    "\n",
    "        loss = outputs[0]  \n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        train_loss += batch_loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if step % 10 == 0 and step > 0:\n",
    "            print('Average train loss: {}'\n",
    "            .format(train_loss / (BATCH_SIZE * step)))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    train_loss = train_loss / len(train_dataloader)       \n",
    "    train_perplexity = 2 ** train_loss\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"     Training Loss: {0:.2f} - Training Perplexity {1:.2f}\".format(train_loss, train_perplexity))\n",
    "        \n",
    "    \"\"\" Validation \"\"\"\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Validating...\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        input = batch[0].to(device)\n",
    "        labels = batch[0].to(device)\n",
    "        mask = batch[1].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            outputs = model(input, labels=labels, attention_mask=mask, token_type_ids=None)          \n",
    "            loss = outputs[0]  \n",
    "            \n",
    "        batch_loss = loss.item()\n",
    "        eval_loss += batch_loss        \n",
    "\n",
    "    eval_loss = eval_loss / len(test_dataloader)\n",
    "    eval_perplexity = (2 ** eval_loss)\n",
    "    \n",
    "    print(\"     Validation Loss: {0:.2f} - Validation Perplexity {1:.2f}\".format(eval_loss, eval_perplexity))\n",
    "\n",
    "    sample(model, 10)\n",
    "    \n",
    "    training_stats.append({'epoch': epoch + 1, \n",
    "                           'Training Loss': train_loss, \n",
    "                           'Training Perplexity': train_perplexity,\n",
    "                           'Valid. Loss': eval_perplexity,\n",
    "                           'Valid. Perplexity': eval_perplexity\n",
    "                           })\n",
    "    \n",
    "    model.save_pretrained(MODEL_PATH + MODEL_NAME + '_epoch_{}.pt'.format(epoch))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "yNy7f89Wc9xH"
   },
   "source": [
    "## Save model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "bXQPllTPc9R7"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(MODEL_PATH+MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "DQzdFLhTcDIa"
   },
   "source": [
    "## Save training stats to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "33i2pFO2cCPD"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "print(\"Saving training stats to disk...\")\n",
    "\n",
    "with open('training_stats.pickle', 'wb') as file:\n",
    "    pickle.dump(training_stats, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "nkbf8stvcXQ4"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     10
    ],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "hidden": true,
    "id": "uSop6p7kaWLW",
    "outputId": "32935cc0-e1a6-436d-a6db-9ae1d1bab279"
   },
   "outputs": [],
   "source": [
    "\"\"\" Testing \"\"\"\n",
    "print(\"\")\n",
    "print(\"Testing...\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "\n",
    "# Evaluate data for one epoch\n",
    "for batch in test_dataloader:\n",
    "    \n",
    "    input = batch[0].to(device)\n",
    "    labels = batch[0].to(device)\n",
    "    mask = batch[1].to(device)\n",
    "    \n",
    "    with torch.no_grad():        \n",
    "        outputs = model(input, labels=labels, attention_mask=mask, token_type_ids=None)          \n",
    "        loss = outputs[0]  # recovers loss from GPT-2 output\n",
    "        \n",
    "    test_loss += loss.item()       \n",
    "\n",
    "test_loss = test_loss / len(test_dataloader)\n",
    "test_perplexity = 2 ** test_loss\n",
    "\n",
    "print(\"     Test Loss: {0:.2f} - Test Perplexity {1:.2f}\".format(test_loss, test_perplexity))\n",
    "\n",
    "testing_stats = {'Test Loss': test_loss, 'Test Perplexity': test_perplexity}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "8YMCQq4CcYc4"
   },
   "source": [
    "## Save testing stats to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "umGbMfC2ca_J"
   },
   "outputs": [],
   "source": [
    "print(\"Saving testing stats to disk...\")\n",
    "\n",
    "with open('testing_stats_epoch_3.pickle', 'wb') as file:\n",
    "    pickle.dump(testing_stats, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "5GiMu7-CdIEa"
   },
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "REGiC34bdTxv"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "with open('training_stats.pickle', 'rb') as file:\n",
    "    training_stats = pickle.load(file)\n",
    "    \n",
    "with open('testing_stats_epoch_3.pickle', 'rb') as file:\n",
    "    testing_stats = pickle.load(file)\n",
    "    \n",
    "train_loss = [data['Training Loss'] for data in training_stats]\n",
    "train_perplexity = [data['Training Perplexity'] for data in training_stats]\n",
    "\n",
    "validation_loss = [data['Training Loss'] for data in training_stats]\n",
    "validation_perplexity = [data['Valid. Perplexity'] for data in training_stats]\n",
    "\n",
    "test_loss = testing_stats['Test Loss']\n",
    "test_perplexity = testing_stats['Test Perplexity']\n",
    "\n",
    "x = range(1, len(training_stats)+1)\n",
    "\n",
    "ax = plt.figure().gca()\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "plt.axhline(test_perplexity, label='test (epochs=3)', color='red')\n",
    "plt.plot(x, train_perplexity, label='train')\n",
    "plt.plot(x, validation_perplexity, label='validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "8QuY9tKkjd6V"
   },
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "JIoYGgClwTVa"
   },
   "outputs": [],
   "source": [
    "sample(model, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ShowerThoughtsGenerator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
